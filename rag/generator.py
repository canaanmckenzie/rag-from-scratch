# rag/generator.py

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import time

# ------------------------------
# SETUP: Load the model and tokenizer
# ------------------------------
# This is your open-source decoder model: f_gen()
# It maps (context + query) to a natural language response
# We use a small instruction-tuned causal LLM
MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.1"

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

# Set the model to evaluation mode (no dropout etc.)
model.eval()

# Make sure pad_token is defined
# Mistral doesn't have one by default, so we use eos_token as pad_token
tokenizer.pad_token = tokenizer.eos_token

# Optional: move model to GPU if available
if torch.cuda.is_available():
    model = model.to("cuda")

# ------------------------------
# FORMAT PROMPT FROM QUERY + CONTEXT
# ------------------------------
def format_prompt(query, docs):
    """
    Format the prompt by combining the user's question with the retrieved documents.
    Each document is a string from the retriever.
    """
    context = "\n\n".join([f"Document {i+1}: {doc}" for i, doc in enumerate(docs)])
    prompt = f"""<s>[INST] Use the following context to answer the question below.\n
{context}

Question: {query}\nAnswer: [/INST]"""
    return prompt

# ------------------------------
# GENERATE RESPONSE
# ------------------------------
def generate_response(query, docs, max_tokens=128):
    """
    Given a query and retrieved docs, generate an answer using the LLM.

    Args:
        query (str): The user's question
        docs (List[str]): Retrieved documents to use as context
        max_tokens (int): Maximum number of new tokens to generate

    Returns:
        str: Natural language answer generated by the model
    """
    # Combine prompt
    prompt = format_prompt(query, docs)

    # Tokenize the prompt
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=2048,
        padding=True
    )

    # Move input tensors to model device (CPU or GPU)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    # Generate response
    with torch.no_grad():
        start_time = time.time()
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            top_p=0.95,
            temperature=0.7,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            use_cache=True,
        )
        duration = time.time() - start_time
        print(f"Generation completed in {duration:.2f} seconds")

    # Decode and extract only the generated answer
    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Heuristic: Extract after "Answer:" if present
    return output_text.split("Answer:")[-1].strip()
